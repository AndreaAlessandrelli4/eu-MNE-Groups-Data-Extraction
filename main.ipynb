{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af08974c",
   "metadata": {},
   "source": [
    "# Eurostat Web-Crawling & Extraction Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6ffdd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "from extraction_utils import ask_chatgpt\n",
    "from navigator_utils import search_google, split_markdown, clean_content\n",
    "from save_utils import convert_for_saving, save_json\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"YOUR_API_KEY_HERE\" # Set your OpenAI API key here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e88008",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27efda9e",
   "metadata": {},
   "source": [
    "This parte of the notebook is the main entry point to run the crawling and LLM-based\n",
    "information extraction pipeline for the Eurostat statistical competition.\n",
    "\n",
    "- It reads the list of companies from `data/extraction_empty.csv`\n",
    "- It crawls the web for each company (annual reports and related pages)\n",
    "- It uses an LLM to extract the target fields (ACTIVITY, COUNTRY, EMPLOYEES, TURNOVER, ASSETS, WEBSITE)\n",
    "- It saves one JSON file per company inside the `save/` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41bc75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = \"./extraction.csv\"  # Input CSV file with company names\n",
    "PROMPT_FILE = \"./data/prompt.json\"  # JSON file containing system and user prompts\n",
    "PATH = \"./data/save_data/\" # Directory to save each company results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0edefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV, sep=\";\", encoding=\"utf-8\")\n",
    "ent_list_ID =  df[\"NAME\"].unique()\n",
    "ent_list_ID_tuple = list(zip(df[\"ID\"].unique(), df[\"NAME\"].unique()))\n",
    "field_key = df['VARIABLE'].unique()\n",
    "id_list = df[\"ID\"].unique()\n",
    "id_list = [str(i) for i in id_list]\n",
    "\n",
    "field =  {'ACTIVITY':'','COUNTRY':'Headquarter country', 'EMPLOYEES':'number of employees', \n",
    "           'TURNOVER':'turnover or  net revenue', 'ASSETS':'total assets', 'WEBSITE':'official website'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259b35e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## loading prompt for extraction\n",
    "with open(PROMPT_FILE, 'r', encoding='utf-8') as file:\n",
    "    diz_prompt =json.load(file)\n",
    "\n",
    "prompt_diz = diz_prompt['PROMPT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cd38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "YEAR = 2024\n",
    "results = {}\n",
    "\n",
    "for id, company_name in tqdm(ent_list_ID_tuple):\n",
    "    results[company_name]= {}\n",
    "    links_ricerca = search_google(company_name, YEAR, field=field, n_pdf=3, n_nopdf=5)\n",
    "    for k, v in links_ricerca.items():\n",
    "        print(f\"Processing {company_name} - {k}\")\n",
    "        if len(v) == 0:\n",
    "            continue\n",
    "        for link in v:\n",
    "            url = link['url']\n",
    "            print(url)\n",
    "            year = link['year']\n",
    "            try:\n",
    "                extract = await clean_content(link['url'])\n",
    "                print('extract!')\n",
    "                if extract.markdown:\n",
    "                    try:\n",
    "                        markdown = extract.markdown.fit_markdown\n",
    "                        if len(markdown) <= 1:\n",
    "                            continue\n",
    "                        elif len(markdown) < 10000:\n",
    "                            chat_extract = ask_chatgpt(link, markdown, user_prompt=prompt_diz[k].replace(\"{T}\", str(link['year'])))\n",
    "                            if 'null' not in chat_extract:\n",
    "                                results[company_name][k] = [str(id), url, chat_extract, str(year)]\n",
    "                                print(results[company_name][k])\n",
    "                                break\n",
    "                        else:\n",
    "                            list_markdown=split_markdown(markdown)\n",
    "                            for l in list_markdown:\n",
    "                                chat_extract = ask_chatgpt(link, l, user_prompt=prompt_diz[k].replace(\"{T}\", str(link['year'])))\n",
    "                                if 'null' not in chat_extract:\n",
    "                                    results[company_name][k] = [str(id), url, chat_extract, str(year)]\n",
    "                                    print(results[company_name][k])\n",
    "                                    break\n",
    "                        if 'null' not in chat_extract:\n",
    "                            break\n",
    "                    except:\n",
    "                        continue\n",
    "                else:\n",
    "                    continue\n",
    "            except:\n",
    "                continue\n",
    "    with open(PATH+f\"ris_{id}_{company_name.replace('/','-')}.json\", \"w\", encoding='utf-8') as f:\n",
    "        json.dump(results[company_name], f, indent=4, ensure_ascii=False)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e8485e",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6000197b",
   "metadata": {},
   "source": [
    "This part of the notebook aggregates the results of the previous part to create a complete csv file with all the extracted information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dafdaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading all data saved into a single dictionary\n",
    "list = [f for f in os.listdir(PATH) if f.endswith('.json')]\n",
    "list_id = [f.split('_')[1] for f in list ]\n",
    "\n",
    "diz = {}\n",
    "for idf, file in enumerate(list):\n",
    "    path = PATH+file\n",
    "    with open(path, \"r\") as fi:\n",
    "        try:\n",
    "            diz[list_id[idf]]=json.load(fi)\n",
    "        except:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931bf35c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOT = convert_for_saving(diz, df)\n",
    "save_json(CSV, TOT, df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
